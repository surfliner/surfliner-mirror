stages:
  - maintanence
  - build
  - lint
  - test
  - integration
  - review
  - staging
  - production
  - result_notification

include:
  - local: ci/base.yml
  - local: ci/comet.yml
  - local: ci/lark.yml
  - local: ci/shoreline.yml
  - local: ci/starlight.yml
  - local: ci/superskunk.yml
  - local: ci/surfliner_schema.yml
  - local: ci/tidewater.yml

# Build with Kaniko. When building on `trunk`, tag a new stable image.
.build_helpers: &build_helpers |
  set -x
  build_image_for() {
      if test $# -lt 2 ; then
        echo "You must specify a project path to a Dockerfile and an image name, and an optional target."
        echo "example: build_image_for 'starlight' 'starlight_web' 'production'"
      exit 1
    fi
    project=$1
    image_name=$2
    target=$3
    destination="--destination $CI_REGISTRY_IMAGE/$image_name:$CI_COMMIT_SHA"
    if [ "$CI_COMMIT_BRANCH" = "trunk" ]; then
      destination="$destination --destination $CI_REGISTRY_IMAGE/$image_name:stable"
    elif [ "$CI_COMMIT_TAG" ]; then
      destination="$destination --destination $CI_REGISTRY_IMAGE/$image_name:$CI_COMMIT_TAG"
    fi

    if [ "$target" ]; then
      /kaniko/executor --context "$CI_PROJECT_DIR" --cache=true --target "$target" --dockerfile "$CI_PROJECT_DIR"/"$project"/Dockerfile $destination
    else
      /kaniko/executor --context "$CI_PROJECT_DIR" --cache=true --dockerfile "$CI_PROJECT_DIR"/"$project"/Dockerfile $destination
    fi
  }


# Use Kaniko for building container images
.kaniko-build:
  stage: build
  image:
    name: gcr.io/kaniko-project/executor:v1.7.0-debug
    entrypoint: [""]
  before_script:
    - *build_helpers
    - echo "{\"auths\":{\"$CI_REGISTRY\":{\"username\":\"$CI_REGISTRY_USER\",\"password\":\"$CI_REGISTRY_PASSWORD\"}}}" > /kaniko/.docker/config.json

# Default only -> refs settings for all jobs
# see: https://docs.gitlab.com/ee/ci/yaml/#onlyrefsexceptrefs
.only-refs-default:
  only:
    refs:
      - trunk
      - merge_requests
      - tags

  # Helm 3 deployment helper functions
.helm3_deploy_helpers: &helm3_deploy_helpers |
  function deploy() {
    chart="${1}"

    if [ -d "${chart}" ]; then
      pushd "$(pwd)"
      cd "${chart}" || exit 1
      download_chart_dependencies
      popd
    fi

    decode_helm_values_files
    if helm ls --namespace="$KUBE_NAMESPACE" -q | grep -q "$HELM_RELEASE_NAME"; then
      echo "Deploying new release: $HELM_RELEASE_NAME..."
    else
      first_release="yes"
      echo "Deploying initial release: $HELM_RELEASE_NAME..."
    fi
    helm upgrade \
        --install \
        --atomic \
        --timeout 15m0s \
        --debug \
        --set image.repository="$DEPLOY_IMAGE" \
        --set image.tag="$DEPLOY_TAG" \
        $HELM_UPGRADE_FILE_VALUES_ARGS \
        $HELM_UPGRADE_EXTRA_ARGS \
        --namespace="$KUBE_NAMESPACE" \
        --create-namespace \
        "$HELM_RELEASE_NAME" \
        "${chart}"
  }

  # Run Helm chart tests for a deployment
  # see: https://helm.sh/docs/topics/chart_tests/
  function run_chart_tests(){
    helm test --namespace="$KUBE_NAMESPACE" "$HELM_RELEASE_NAME"
  }

  # Install chart dependencies
  function download_chart_dependencies(){
    echo "Downloading Helm chart dependencies.."
    helm repo add bitnami https://charts.bitnami.com/bitnami

    helm dependency build .
  }

  function delete() {
    echo "Deleting release: $HELM_RELEASE_NAME..."
    helm delete --namespace "$KUBE_NAMESPACE" "$HELM_RELEASE_NAME"
  }

  # note: should not be used in production
  function delete_stateful_set_pvcs(){
    echo "Deleting any StatefulSet Persistent Volume Claims..."
    # postgresql and zookeeper
    kubectl delete pvc -l release="$HELM_RELEASE_NAME" --namespace="$KUBE_NAMESPACE"
    # solr
    kubectl delete pvc -l "app.kubernetes.io/instance=$HELM_RELEASE_NAME" --namespace="$KUBE_NAMESPACE"
  }

  delete_rabbitmq_queue() {
    echo "Deleting rabbitmq queue: $RABBITMQ_RELEASE_NAME..."
    kubectl --kubeconfig $KUBECONFIG delete rabbitmqclusters.rabbitmq.com $RABBITMQ_RELEASE_NAME -n $KUBE_NAMESPACE
  }

  function rollback() {
    echo "Rolling back to previous release of: $HELM_RELEASE_NAME..."
    helm rollback --wait --namespace "$KUBE_NAMESPACE" "$HELM_RELEASE_NAME"
  }

  # Support supplying a base64-encoded helm values file for deployment
  function setup_helm_values_file(){
    if [ "$HELM_ENCODED_VALUES" ]; then
      echo "HELM_ENCODED_VALUES CI/CD variable found. Decoding and creating HELM_VALUES_FILE"
      echo ${HELM_ENCODED_VALUES} | base64 -d > $HELM_VALUES_FILE
    fi
  }

  function decode_helm_values_files(){
    COUNTER=0
    if [ "$HELM_ENCODED_VALUES_FILES" ]; then
      for values in $(echo $HELM_ENCODED_VALUES_FILES | sed "s/,/ /g")
      do
        echo "HELM_ENCODED_VALUES_FILES CI/CD variable found. Decoding and creating HELM_UPGRADE_FILE_VALUES_ARGS"
        filename="helm-values-$COUNTER.yaml"
        echo ${values} | base64 -d > $filename
        export HELM_UPGRADE_FILE_VALUES_ARGS="$HELM_UPGRADE_FILE_VALUES_ARGS --values $filename"
        COUNTER=$(( COUNTER+1 ));
      done
    fi
  }

  # A GitLab-integrated Kubernetes cluster should already have a KUBECONFIG created
  # Otherwise, we assume the KUBECONFIG is setup as a base64-encoded environment variable
  function setup_kube_config(){
    if [ -z "${KUBECONFIG}" ]; then
      echo "A GitLab KUBECONFIG file was not detected, using KUBE_ROLE_CONFIG env var.."
      mkdir -p /etc/deploy
      export KUBECONFIG=/etc/deploy/config
      echo ${KUBE_ROLE_CONFIG} | base64 -d > $KUBECONFIG
    fi
  }

# Helm3-based image for tiller-less deploys
.helm3-deploy:
  image: dtzar/helm-kubectl:3.8.0
  stage: deploy
  variables:
    HELM_VALUES_FILE: /helm-values.yml
  before_script:
    - *helm3_deploy_helpers
    - setup_kube_config
    - setup_helm_values_file
